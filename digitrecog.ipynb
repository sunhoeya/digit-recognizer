{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Digit Recognition using Pytorch [Google colab]"
      ],
      "metadata": {
        "id": "i8cG11AAPEoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "McQ4hCFq8Ywr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. import torch = PyTorch is the main deep learning library.\n",
        "2. import torch.nn as nn = PyTorchs sub library to build neural network.\n",
        "3. import torch.optim as optim = contains optimizers for adjusting weights for better performance.\n",
        "4. from torchvision import datasets, transforms  \n",
        "5. torchvision = PyTorchs image toolbox.\n",
        "  * datasets = ready-made image datasets - MNIST digits\n",
        "  * transforms = pre-processing tools (resize, normalize, convert to tensor) to prepare images.\n",
        "\n",
        "6. from torch.utils.data import DataLoader = serves data to the model in small batches instead of all at once which is efficient.\n",
        "7. from google.colab import files = For uploading/downloading files between computer and Colab notebook.\n",
        "8. from PIL import Image = image processing library (open, resize, edit images).\n",
        "9. import matplotlib.pyplot as plt = chart and image display tool, lets you visualize what’s happening."
      ],
      "metadata": {
        "id": "iCHnCLRTPD63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AZsaYeo18dwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed32704a-4118-415f-bf64-fb06f659d92f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 32.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.01MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 8.04MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.25MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**preparing images befor giving it ot the model**\n",
        "\n",
        "1. transform = transforms.Compose = performing the following steps\n",
        "   * transforms.ToTensor() = changes the image from normal picture format to a PyTorch-friendly format ,a tensor. changes pixel number from 0-255to 0-1\n",
        "   * transforms.Normalize((0.5,), (0.5,) = Adjusts numbers so they’re centered around 0 instead of all being positive, helps the model learn faster.\n",
        "\n",
        "2. train_dataset/test_dataset = to get the MNIST dataset\n",
        "   * root='./data'= Put the files in a folder named data in the current location.\n",
        "   * train=True = learning data for the model\n",
        "   * train=False = testing data to check how good the model is.\n",
        "   * download=True = Download if not already there\n",
        "   * transform=transform = Every image will be run through the steps we defined above before use\n",
        "\n",
        "3. train_loader = DataLoader/test_loader = DataLoader\n",
        "   = Take the dataset and give it to the model in small bactches.\n",
        "   * batch_size=64 = 64 images at a time.\n",
        "   * shuffle=True = Randomize training images every time so the model doesn’t just memorize the order.\n",
        "   * shuffle=False = Keep test data in the same order so results are consistent"
      ],
      "metadata": {
        "id": "Yo3yZF1-18oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7w6h5rFS8eza"
      },
      "outputs": [],
      "source": [
        "class DigitClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DigitClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = DigitClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. class DigitClassifier(nn.Module): = Define a neural network class that inherits from PyTorch's nn.Module.\n",
        " * def __init__(self):  = Constructor method that runs when a new instance is created.\n",
        "  * super(DigitClassifier, .. = Call the parent class constructor to initialize the module.\n",
        "  * self.fc1 = nn.Linear(28*28, 128) = Create a fully connected layer mapping 784 inputs to 128 outputs.\n",
        "  * self.. = nn.Linear(128, 64) = mapping 128 inputs to 64 outputs.\n",
        "  * sel... = nn.Linear(64, 10) = mapping 64 inputs to 10 outputs.\n",
        "\n",
        " * def forward(self, x): = Define how input tensors pass through the network to produce outputs.\n",
        "  * x = x.view(-1, 28*28) = Flatten each input image to a 784-length vector while keeping batch size.\n",
        "  * x = torch.relu(self.fc1(x)) = Apply the first linear layer then the ReLU activation function.\n",
        "  * x = torch.relu(self.fc2(x)) = Apply the second linear layer then the ReLU activation function.\n",
        "  * x = self.fc3(x) =Apply the final linear layer to produce raw class scores (logits).\n",
        "  * return x = Return the output logits.\n",
        "\n",
        "2. model .. = Instantiate the model so it can be trained or used for inference."
      ],
      "metadata": {
        "id": "xxpPvEnv6jjP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HfPVaF-p8h4r"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. criterion = nn.CrossEntropyLoss() = defines the loss function for multi-class classification that measures how far predictions are from correct labels\n",
        "2. optimizer = optim.Adam(model.parameters(), lr=0.001) = creates the Adam optimizer to update model parameters with a learning rate of 0.001"
      ],
      "metadata": {
        "id": "kB_uqp1j7PXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-2jkoWTm8kxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d126e9-71d2-4269-d866-ddd79e0a806d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.4045\n",
            "Epoch 2/5 - Loss: 0.1954\n",
            "Epoch 3/5 - Loss: 0.1404\n",
            "Epoch 4/5 - Loss: 0.1120\n",
            "Epoch 5/5 - Loss: 0.0938\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. epochs = 5 = number of times the model will see the entire training dataset  \n",
        "2. for epoch in range(epochs): = loop through the dataset once per epoch  \n",
        " * model.train() = put the model in training mode\n",
        " * running_loss = 0 = start tracking the total loss for this epoch  \n",
        " * for images, labels in train_loader: = loop over mini-batches of training data and labels  \n",
        " * optimizer.zero_grad() = reset previous gradients to avoid accumulation  \n",
        " * outputs = model(images) = run the images through the model to get predictions  \n",
        " * loss = criterion(outputs, labels) = calculate how wrong the predictions are using the loss function  \n",
        " * loss.backward() = compute the gradients for each model parameter  \n",
        " * optimizer.step() = update the model parameters using the computed gradients  \n",
        " * running_loss += loss.item() = add this batch’s loss value to the total running loss  \n",
        "\n",
        "3. avg_loss = running_loss / len(train_loader) = calculate avg loss over all batches  \n",
        "4. print.. = display the epoch number and average loss formatted to 4 decimal places  "
      ],
      "metadata": {
        "id": "11KzwyrW8ZX3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FGyO2QNG8nIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b27ec47-b11b-4274-bb7d-1d195a39bcc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 96.58%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. model.eval() = set the model to evaluation mode\n",
        "2. correct = 0 = initialize counter for correct predictions  \n",
        "3. otal = 0 = initialize counter for total test samples  \n",
        "4. with torch.no_grad(): = disable gradient calculation,saves memory and speeds up testing  \n",
        " * for images, labels in test_loader: = loop through batches of test images and labels  \n",
        " * outputs = model(images) = get predictions from the model  \n",
        " * _, predicted = torch.max(outputs, 1) = find the class index with the highest score for each sample  \n",
        " * total += labels.size(0) = add the number of samples in this batch to the total counter  \n",
        " * correct += (predicted == labels).sum().item() = add the number of correct predictions to the correct counter  \n",
        "\n",
        "5. accuracy = 100 * correct / total = calculate the percentage of correctly predicted samples  \n",
        "6. print = display the test accuracy formatted to two decimal places  \n"
      ],
      "metadata": {
        "id": "K9iW0Jvl8YW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "tgXV5ush8p1E",
        "outputId": "a7203d17-165b-450a-de4a-07ca81ea359f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-19990fa9-761b-4854-ab23-15249c789f75\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-19990fa9-761b-4854-ab23-15249c789f75\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test5.png to test5.png\n",
            "Uploaded file: test5.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Digit: 6\n"
          ]
        }
      ],
      "source": [
        "def process_image(image_path):\n",
        "    img = Image.open(image_path).convert('L')\n",
        "    img = Image.eval(img, lambda x: 255 - x)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    img = transform(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    return img\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Uploaded file: {filename}\")\n",
        "    img_tensor = process_image(filename)\n",
        "\n",
        "    plt.imshow(img_tensor.squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        print(f\"Predicted Digit: {pred.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. def process_image(image_path): = Preparing image for the model  \n",
        "2. img = Image.open(image_path).convert('L') = open image and convert it to grayscale  \n",
        "3. img = Image.eval(img, lambda x: 255 - x) = invert image colors so digits are white on black  \n",
        "4. transform = transforms.Compose = create a sequence of transformations to prepare the image  \n",
        "  * transforms.Resize((28, 28)) = resize the image to 28×28 pixels  \n",
        "  * transforms.ToTensor() = convert the image to a PyTorch tensor  \n",
        "  * transforms.Normalize((0.5,), (0.5,)) = normalize pixel values to range [-1, 1]\n",
        "    \n",
        "  * img = transform(img) = apply transformations to image  \n",
        "  * img = img.unsqueeze(0) = add batch dimension so model sees it as one sample  \n",
        "  * return img = return the processed image tensor  \n",
        "\n",
        "5. uploaded = files.upload() = open a file upload dialog in Colab for user to select an image  \n",
        "\n",
        " * for filename in uploaded.keys(): = loop over each uploaded file  \n",
        " * print.. = display file name  \n",
        " * img_tensor = process_image(filename) = process uploaded image for model input  \n",
        "\n",
        " * plt.imshow(img_tensor.squeeze(), cmap='gray') = show processed image without the batch dimension in grayscale  \n",
        " * plt.axis('off') = hide axis for cleaner display  \n",
        " * plt.show() = display image  \n",
        "\n",
        " * model.eval() = set model to evaluation mode  \n",
        " * with torch.no_grad(): = disable gradient calculation for faster inference  \n",
        " * output = model(img_tensor) = pass img through model to get predictions\n",
        " * _, pred = torch.max(output, 1) = get the index of the highest score (predicted digit)\n",
        " * print.. = print predicted digit as num"
      ],
      "metadata": {
        "id": "khT4BsFK81mk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}